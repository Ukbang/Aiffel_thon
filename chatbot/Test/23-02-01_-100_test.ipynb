{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e369af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b18a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "class cfg:\n",
    "    user_token = \"<usr>\"\n",
    "    bot_token = \"<sys>\"\n",
    "    bos_token = '<s>'\n",
    "    eos_token = '</s>'\n",
    "    mask_token = '<mask>'\n",
    "    pad_token = '<pad>'\n",
    "    unk_token = '<unk>'\n",
    "    max_len = 256\n",
    "    max_turns = 6\n",
    "    pre_epochs = 0\n",
    "    epochs = 5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    learning_rate = 1e-4\n",
    "    model_name = \"skt/kogpt2-base-v2\"\n",
    "    labeling_type = True\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(cfg.model_name,\n",
    "            bos_token=cfg.bos_token, eos_token=cfg.eos_token, unk_token=cfg.unk_token,\n",
    "            pad_token=cfg.pad_token, mask_token=cfg.mask_token, model_max_length = cfg.max_len)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796530d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "path = './check_point/2023-02-01-01h-15m_epoch_5_sk_labeling_-100.pth' # 말을 짧게함\n",
    "model.load_state_dict(torch.load(path))\n",
    "print('완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9aba098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user > 점심 먹었어?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:874: UserWarning: Both `max_length` and `max_new_tokens` have been set but they serve the same purpose.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot >  아직\n",
      "user > 어떤 음식 좋아해?\n",
      "Chatbot > ?\n",
      "user > 초기화\n",
      "user > 운동 좋아해?\n",
      "Chatbot >  아니야\n",
      "user > 그럼 뭐 좋아해?\n",
      "Chatbot >  운동?\n",
      "user > 카페는?\n",
      "Chatbot >  오빤?\n",
      "user > 난 카페 좋아해\n",
      "Chatbot >  ᄏᄏ\n",
      "user > 끝\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "대화 함수\n",
    "끝으로 대화 종료\n",
    "초기화로 이전 대화 초기화\n",
    "\n",
    "이전 대화가 history에 남아있음.\n",
    "\"\"\"\n",
    "\n",
    "history = \"\"\n",
    "start=True\n",
    "model.to(cfg.device)\n",
    "\n",
    "while True:\n",
    "    _input = input(\"user > \")\n",
    "    \n",
    "    if _input == \"끝\":\n",
    "        break\n",
    "    if _input == \"초기화\":\n",
    "        history = \"\"\n",
    "        continue\n",
    "    if start:\n",
    "        _input_word = cfg.bos_token + cfg.user_token + _input + cfg.bot_token\n",
    "        history += _input_word\n",
    "        start = False\n",
    "    else:\n",
    "        _input_word = cfg.user_token + _input + cfg.bot_token\n",
    "        history += _input_word\n",
    "        \n",
    "    input_ids = tokenizer.encode(history, return_tensors=\"pt\").to(cfg.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            input_ids,\n",
    "            top_k=3,\n",
    "            top_p=0.92,\n",
    "            num_beams=3,\n",
    "            do_samples=True,\n",
    "            max_length=1000,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1.2,\n",
    "            temperature=0.85,\n",
    "            max_new_tokens=30,\n",
    "            eos_token_id= tokenizer.get_vocab()[cfg.eos_token],\n",
    "            bad_word_ids = [tokenizer.convert_tokens_to_ids(cfg.user_token), tokenizer.convert_tokens_to_ids('ㅋㅋ'), tokenizer.convert_tokens_to_ids('하하')]\n",
    "        )\n",
    "    \n",
    "    gen = tokenizer.decode(gen_ids[0])\n",
    "    try:\n",
    "        generated = gen[gen.rfind(\"<sys>\")+5:gen.index(\"</s>\")]\n",
    "    except:\n",
    "        generated = gen[gen.rfind(\"<sys>\")+5:]\n",
    "    history += generated\n",
    "    \n",
    "    #print(f'Chatbot > {gen}')\n",
    "    print(f'Chatbot > {generated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e6e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = '점심 먹었어?'\n",
    "input_word = cfg.bos_token + cfg.user_token + _input + cfg.bot_token\n",
    "input_ids = tokenizer.encode(input_word, return_tensors=\"pt\").to(cfg.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f923de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 심시간에었어?</s> 아직'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(*torch.argmax(outputs.logits, dim=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
