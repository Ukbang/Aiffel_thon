{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96f0e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aecc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_token = \"<user>\"\n",
    "bot_token = \"<bot>\"\n",
    "MASK = '<mask>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf7d29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=bot_token, eos_token=user_token, unk_token='<unk>',\n",
    "            pad_token=PAD, mask_token=MASK)\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c236f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed900de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/multi_turn_tokens.csv') # 적정길이 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f845505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/341630 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (203 > 200). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 341630/341630 [03:33<00:00, 1602.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    if tokenizer.model_max_length < len(tokenizer.encode(data['conversation'][i])):\n",
    "        data['conversation'][i] = np.NaN\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae85a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conversation    29961\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a95b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e54686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, tokenizer):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
    "        token = self._data['conversation'][idx]\n",
    "        turn = tokenizer.tokenize(token)\n",
    "        turn = tokenizer.convert_tokens_to_ids(turn)\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(token, padding='max_length')\n",
    "        \n",
    "        mask = []\n",
    "        labels = []\n",
    "        for i in turn:\n",
    "            if i == tokenizer.eos_token_id: loop = True\n",
    "            if i == tokenizer.sep_token_id: loop = False\n",
    "            if loop == True: \n",
    "                mask += [0]\n",
    "            if loop == False:\n",
    "                mask += [1]\n",
    "                \n",
    "        for i,v in enumerate(turn):\n",
    "            if i == 0 and v == tokenizer.eos_token_id:\n",
    "                labels.append(tokenizer.mask_token)\n",
    "                continue\n",
    "            if v == tokenizer.eos_token_id: loop = True\n",
    "            if v == tokenizer.sep_token_id: loop = False\n",
    "            if loop == False:\n",
    "                labels.append(tokenizer.convert_ids_to_tokens(v))\n",
    "            if loop == True and v == tokenizer.eos_token_id:\n",
    "                labels.append(tokenizer.eos_token)\n",
    "            elif loop == True:\n",
    "                labels.append(tokenizer.mask_token)\n",
    "        \n",
    "        labels_tokens = \"\"\n",
    "        for i in labels: labels_tokens += i\n",
    "        loop = True\n",
    "        while loop:\n",
    "            if len(mask) < tokenizer.model_max_length:\n",
    "                mask.append(tokenizer.pad_token_id)\n",
    "            else: loop = False\n",
    "                \n",
    "#         mask = torch.LongTensor(mask)\n",
    "#         mask = mask.view(1, tokenizer.model_max_length)\n",
    "        \n",
    "        labels_ids = self.tokenizer.encode(labels_tokens, padding='max_length')\n",
    "        return token_ids, mask, labels_ids\n",
    "\n",
    "def collate_batch(batch):\n",
    "    data =  [item[0] for item in batch]\n",
    "    mask =  [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294069c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ChatbotDataset(data, tokenizer)\n",
    "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, num_workers=2, shuffle=True, collate_fn=collate_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b524f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "token_ids ====>  tensor([[51200,  9893,  7991,  ...,     3,     3,     3],\n",
      "        [51200, 18519,  9285,  ...,     3,     3,     3],\n",
      "        [51200, 10489,  9190,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [51200,  9664,  7293,  ...,     3,     3,     3],\n",
      "        [51200,  9114,  7979,  ...,     3,     3,     3],\n",
      "        [51200,  9769,  9497,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])\n",
      "label =====>  tensor([[6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[51200, 18948, 24060,  ...,     3,     3,     3],\n",
      "        [51200, 10099,  6824,  ...,     3,     3,     3],\n",
      "        [51200, 46905,  6874,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [51200, 10287, 16686,  ...,     3,     3,     3],\n",
      "        [51200, 13919,  8599,  ...,     3,     3,     3],\n",
      "        [51200,  9676,  7970,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])\n",
      "label =====>  tensor([[6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[51200,  9198, 23775,  ...,     3,     3,     3],\n",
      "        [51200,  9063,  9046,  ...,     3,     3,     3],\n",
      "        [51200,  9376,  7692,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [51200,  9050,  9208,  ...,     3,     3,     3],\n",
      "        [51200, 22240, 11732,  ...,     3,     3,     3],\n",
      "        [51200, 20058,  9177,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])\n",
      "label =====>  tensor([[6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[51200, 14391, 12348,  ...,     3,     3,     3],\n",
      "        [51200, 10896, 13717,  ...,     3,     3,     3],\n",
      "        [51200, 42601,  6918,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [51200,  9252,  7220,  ...,     3,     3,     3],\n",
      "        [51200, 17932, 18328,  ...,     3,     3,     3],\n",
      "        [51200, 10452, 27392,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])\n",
      "label =====>  tensor([[6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3],\n",
      "        [6, 6, 6,  ..., 3, 3, 3]])\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for batch_idx, samples in enumerate(train_dataloader):\n",
    "    if batch_idx > 3:\n",
    "        break\n",
    "    token_ids, mask, label = samples\n",
    "    print(\"token_ids ====> \", token_ids)\n",
    "    print(\"mask =====> \", mask)\n",
    "    print(\"label =====> \", label)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef100999",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "epochs = 1\n",
    "Sneg = -1e18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b847c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input_embeddings(torch.nn.Embedding(51201, 768))\n",
    "model.set_output_embeddings(torch.nn.Linear(768, 51201, bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32143daa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1305/4040528363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e462e2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "0\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1305/3365539002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import time\n",
    "print (\"start\")\n",
    "s_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids, mask, label = samples\n",
    "        token_ids = token_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(token_ids)\n",
    "        out = out.logits      #Returns a new tensor with the logit of the elements of input\n",
    "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
    "        loss = criterion(mask_out.transpose(2, 1), label)\n",
    "        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n",
    "        avg_loss = loss.sum() / mask.sum()\n",
    "        avg_loss.backward()\n",
    "        # 학습 끝\n",
    "        optimizer.step()\n",
    "    e_time = time.time()\n",
    "    print(f'{epoch}번째 학습까지 걸린시간 {e_time-s_time}')\n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e646c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
